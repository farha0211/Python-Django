{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farha0211/Python-Django/blob/master/nl2code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pukn5f5iNdds"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNbF2PyLNddx"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from parameters import parse_args\n",
        "import sys; sys.argv=['']; del sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fmAstCkENddy",
        "outputId": "2dd72ca2-0b26-4663-90a9-0afef97e9c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'constants'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dae675dc9317>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from constants import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mLLMsMap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mEmbeddingModelsMap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'constants'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from constants import (\n",
        "    LLMsMap,\n",
        "    EmbeddingModelsMap,\n",
        ")\n",
        "\n",
        "from api_models import set_llm_and_embed\n",
        "args = parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtwrCmDyNddz",
        "outputId": "757af16e-1a2d-4a9a-c8e3-88fee22af2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using LLM: meta-llama/Meta-Llama-3-70B-Instruct\n",
            "Using Embedding Model: BAAI/bge-m3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "args.embed_model = 'bge_m3'\n",
        "\n",
        "llm_name = LLMsMap[args.llm]\n",
        "embed_model_name = EmbeddingModelsMap[args.embed_model]\n",
        "print(f\"Using LLM: {llm_name}\")\n",
        "print(f\"Using Embedding Model: {embed_model_name}\")\n",
        "\n",
        "set_llm_and_embed(\n",
        "    llm_type=args.llm_type,\n",
        "    llm_name=llm_name,\n",
        "    embed_model_name=embed_model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adpce7ZwNdd1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_dataset_nodes(\n",
        "    dataset_name,\n",
        "    path: str = 'indexed_nodes',\n",
        "    use_mc: bool = False,\n",
        "    use_summary: bool = False\n",
        "):\n",
        "    indexed_nodes = pickle.load(open(f'{path}/{dataset_name}{\"_mc\" if use_mc else \"\"}{\"_sm\" if use_summary else \"\"}.pkl', 'rb'))\n",
        "    print(f\"Number of indexed nodes: {len(indexed_nodes)}\")\n",
        "    req_nodes = pickle.load(open(f'similar_requirements/{dataset_name}.pkl', 'rb'))\n",
        "    print(f\"Number of requirements nodes: {len(req_nodes)}\")\n",
        "    return indexed_nodes, req_nodes\n",
        "\n",
        "# indexed_nodes, req_nodes = get_dataset_nodes(dataset_name, 'indexed_nodes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N4H0BFb8Ndd1",
        "outputId": "1a032b94-afd9-4ce9-b3f9-4a834cb81388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index.retrievers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-00c1a750d766>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSemanticSimilarityEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrievers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBM25Retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.retrievers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
        "from llama_index.core import Settings\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.indices import VectorStoreIndex\n",
        "from collections import defaultdict\n",
        "from code2graph import CLASS_NAME_LABEL, get_docs_nxg\n",
        "from typing import List\n",
        "from typing import Union\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore, Document\n",
        "from constants import BM25_INDEX_RETREIVER, VECTOR_INDEX_RETREIVER\n",
        "from evaluation import evaluate_response, get_solutions\n",
        "from indexing.constants import (\n",
        "    CLASS_NAME_LABEL,\n",
        "    DOCSTRING_LABEL,\n",
        "    ATTRIBUTES_LABEL,\n",
        "    ATTRIBUTE_NAME_LABEL,\n",
        "    ATTRIBUTES_TYPE_LABEL,\n",
        "    METHOD_NAME_LABEL,\n",
        "    METHODS_LABEL,\n",
        ")\n",
        "from llama_index.core import QueryBundle\n",
        "from retrievers import get_reachable_nodes\n",
        "\n",
        "\n",
        "def get_dataset_nodes(\n",
        "    dataset_name,\n",
        "    path: str = 'indexed_nodes',\n",
        "    use_mc: bool = False,\n",
        "    use_summary: bool = False\n",
        "):\n",
        "    indexed_nodes = pickle.load(open(f'{path}/{dataset_name}{\"_mc\" if use_mc else \"\"}{\"_sm\" if use_summary else \"\"}.pkl', 'rb'))\n",
        "    print(f\"Number of indexed nodes: {len(indexed_nodes)}\")\n",
        "    req_nodes = pickle.load(open(f'similar_requirements/{dataset_name}.pkl', 'rb'))\n",
        "    print(f\"Number of requirements nodes: {len(req_nodes)}\")\n",
        "    return indexed_nodes, req_nodes\n",
        "\n",
        "def get_graph_node_str(graph_node):\n",
        "    content = f\"Class {graph_node[CLASS_NAME_LABEL]}\\n\"\n",
        "    if graph_node[DOCSTRING_LABEL]:\n",
        "        content += f\"Docstring: {graph_node[DOCSTRING_LABEL]}\\n\"\n",
        "\n",
        "    # if ATTRIBUTES_LABEL in graph_node and len(graph_node[ATTRIBUTES_LABEL]):\n",
        "    #     content += f\"Attributes: \\n\"\n",
        "    #     for attr in graph_node[ATTRIBUTES_LABEL]:\n",
        "    #         content += f\"{attr[ATTRIBUTE_NAME_LABEL]}: {attr[ATTRIBUTES_TYPE_LABEL]}\\n\"\n",
        "\n",
        "    # method_names = [i[METHOD_NAME_LABEL] for i in graph_node[METHODS_LABEL]]\n",
        "    # if len(method_names):\n",
        "    #     content += f\"Methods: {', '.join(method_names)}\"\n",
        "    return content\n",
        "\n",
        "\n",
        "def get_graph_node_str(graph_node):\n",
        "    content = f\"Class {graph_node[CLASS_NAME_LABEL]}\\n\"\n",
        "    if graph_node[DOCSTRING_LABEL]:\n",
        "        content += f\"Docstring: {graph_node[DOCSTRING_LABEL]}\\n\"\n",
        "\n",
        "    # if ATTRIBUTES_LABEL in graph_node and len(graph_node[ATTRIBUTES_LABEL]):\n",
        "    #     content += f\"Attributes: \\n\"\n",
        "    #     for attr in graph_node[ATTRIBUTES_LABEL]:\n",
        "    #         content += f\"{attr[ATTRIBUTE_NAME_LABEL]}: {attr[ATTRIBUTES_TYPE_LABEL]}\\n\"\n",
        "\n",
        "    # method_names = [i[METHOD_NAME_LABEL] for i in graph_node[METHODS_LABEL]]\n",
        "    # if len(method_names):\n",
        "    #     content += f\"Methods: {', '.join(method_names)}\"\n",
        "    return content\n",
        "\n",
        "\n",
        "\n",
        "class NL2CodeTracer(BaseRetriever):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name,\n",
        "        retrieval_distance: int = 1,\n",
        "        similarity_threshold: float = 0.6,\n",
        "        base_dir='data_repos/ftlr/datasets',\n",
        "        chroma_db_dir='indices',\n",
        "        solutions_file='solution_links_english.txt',\n",
        "        call_graph_file='method_callgraph.json',\n",
        "        all_code_files_path='all_code_filenames.txt',\n",
        "        all_req_file_names='all_req_filenames.txt',\n",
        "    ):\n",
        "\n",
        "        self.sem_evaluator = SemanticSimilarityEvaluator(\n",
        "            embed_model=Settings.embed_model,\n",
        "            similarity_threshold=similarity_threshold,\n",
        "        )\n",
        "        self.retrieval_distance = retrieval_distance\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        self.all_code_files_path = all_code_files_path\n",
        "        self.all_req_file_names = all_req_file_names\n",
        "\n",
        "        self.base_dir = base_dir\n",
        "        self.dataset_dir = f'{base_dir}/{dataset_name}'\n",
        "        indices_path = f\"{chroma_db_dir}/{dataset_name}\"\n",
        "        os.makedirs(indices_path, exist_ok=True)\n",
        "\n",
        "        self.solutions_file_path = f'{self.dataset_dir}/{dataset_name.lower()}_{solutions_file}'\n",
        "        self.call_graph_file = f'{dataset_name.lower()}_{call_graph_file}'\n",
        "        self.class_names2node_map = None\n",
        "\n",
        "\n",
        "    def set_dataset_data(\n",
        "            self,\n",
        "            use_mc=False,\n",
        "            use_summary=False,\n",
        "            use_similar_q=False\n",
        "        ):\n",
        "        self.use_mc = use_mc\n",
        "        self.use_summary = use_summary\n",
        "        self.use_similar_q = use_similar_q\n",
        "\n",
        "        indexed_nodes, req_nodes = get_dataset_nodes(\n",
        "            self.dataset_name,\n",
        "            'indexed_nodes',\n",
        "            use_mc=use_mc,\n",
        "            use_summary=use_summary\n",
        "        )\n",
        "\n",
        "        if self.use_similar_q:\n",
        "            for i in range(len(req_nodes)):\n",
        "                similar_qs = '\\n'.join(req_nodes[i].metadata['similar_queries'][:2])\n",
        "                req_nodes[i].text += f\"\\n\\nSet of similar requirements as context: \\n{similar_qs}\"\n",
        "\n",
        "        self.indexed_nodes = indexed_nodes\n",
        "        self.req_nodes = req_nodes\n",
        "\n",
        "        self.set_class_node_maps()\n",
        "        self.set_docs_nxg_and_graph_nodes()\n",
        "        self.set_solution_links()\n",
        "\n",
        "\n",
        "    def set_solution_links(self):\n",
        "        solutions = get_solutions(self.solutions_file_path)\n",
        "        self.req_file_to_node_map = {\n",
        "            n.metadata['file_name']: solutions[n.metadata['file_name']]\n",
        "            for n in self.req_nodes\n",
        "        }\n",
        "\n",
        "\n",
        "    def set_class_node_maps(self):\n",
        "        self.node_map = {n.hash: n for n in self.indexed_nodes}\n",
        "        self.class_names2node_map = defaultdict(list)\n",
        "        for n in self.indexed_nodes:\n",
        "            self.class_names2node_map[n.metadata[CLASS_NAME_LABEL]].append(n.hash)\n",
        "\n",
        "\n",
        "    def set_docs_nxg_and_graph_nodes(self):\n",
        "        self.docs_nxg, self.graph_class_nodes = get_docs_nxg(\n",
        "            dataset_dir=self.dataset_dir,\n",
        "            all_code_files_path=self.all_code_files_path,\n",
        "            callgraph_file_name=self.call_graph_file\n",
        "        )\n",
        "\n",
        "    async def get_semantic_score(self, query_str, doc_str):\n",
        "        # print(self.sem_evaluator._embed_model)\n",
        "        result = await self.sem_evaluator.aevaluate(response=doc_str, reference=query_str)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        # print(\"Query: \", query_bundle.query_str)\n",
        "        retrieved_nodes: List[NodeWithScore] = []\n",
        "        for retriever in self.retrievers:\n",
        "            retrieved_nodes.extend(retriever.retrieve(query_bundle.query_str))\n",
        "\n",
        "        retrieved_nodes = list({n.node_id: n for n in retrieved_nodes}.values())\n",
        "\n",
        "        class_names = list(set([n.metadata['Class Name'] for n in retrieved_nodes]))\n",
        "        assert all(n in self.docs_nxg for n in class_names)\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"Class names\", class_names)\n",
        "\n",
        "        reachable_classes = list(\n",
        "            set(sum([\n",
        "                get_reachable_nodes(self.docs_nxg, class_name, self.retrieval_distance)\\\n",
        "                for class_name in class_names], []\n",
        "            ))\n",
        "        )\n",
        "        node_with_scores = list()\n",
        "        for c in reachable_classes:\n",
        "            if c not in self.graph_class_nodes:\n",
        "                continue\n",
        "            class_node_str = get_graph_node_str(self.graph_class_nodes[c])\n",
        "            doc = self.node_map[self.class_names2node_map[c][0]]\n",
        "            if 'section_summary' in doc.metadata:\n",
        "                class_node_str += f\"\\n\\nSummary: {doc.metadata['section_summary']}\"\n",
        "\n",
        "            node = Document(\n",
        "                text=class_node_str,\n",
        "                metadata=doc.metadata,\n",
        "                excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n",
        "                excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys\n",
        "            )\n",
        "            sim_result = asyncio.run(self.get_semantic_score(query_bundle.query_str, class_node_str))\n",
        "\n",
        "            if sim_result.passing:\n",
        "                node_with_score = NodeWithScore(node=node, score=sim_result.score)\n",
        "                node_with_scores.append(node_with_score)\n",
        "\n",
        "        # print(\"Retrieved nodes: \", len(node_with_scores))\n",
        "        return node_with_scores\n",
        "\n",
        "\n",
        "    def set_retrivers(\n",
        "        self,\n",
        "        types: List[str],\n",
        "        similarity_top_k: int = 2,\n",
        "    ):\n",
        "        self.retrievers: Union[BaseRetriever, List[BaseRetriever]] = []\n",
        "        if VECTOR_INDEX_RETREIVER in types:\n",
        "            vector_index = VectorStoreIndex(nodes=self.indexed_nodes, show_progress=True)\n",
        "            vector_retriever = vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
        "            self.retrievers.append(vector_retriever)\n",
        "\n",
        "        if BM25_INDEX_RETREIVER in types:\n",
        "            bm25_retriever = BM25Retriever.from_defaults(\n",
        "            nodes=self.indexed_nodes,\n",
        "            similarity_top_k=similarity_top_k,\n",
        "        )\n",
        "            self.retrievers.append(bm25_retriever)\n",
        "\n",
        "\n",
        "    def evaluate_retrievers(self, retrievers: List[str], both=False):\n",
        "        self.set_retrivers(retrievers)\n",
        "        retrievers_str = '_'.join(retrievers)\n",
        "\n",
        "        qes = {f'{retrievers_str}_nl': RetrieverQueryEngine(self)}\n",
        "\n",
        "        if both:\n",
        "            qes[retrievers_str] = RetrieverQueryEngine(self.retrievers[0])\n",
        "\n",
        "\n",
        "\n",
        "        correctness_results = evaluate_response(\n",
        "            req_nodes=self.req_nodes,\n",
        "            query_engines=qes,\n",
        "            solutions_file=self.solutions_file_path,\n",
        "            dataset_name=self.dataset_name\n",
        "        )\n",
        "        mc = '_mc' if self.use_mc else ''\n",
        "        sm = '_sm' if self.use_summary else ''\n",
        "        eq = '_eq' if self.use_similar_q else ''\n",
        "        with open(f'results/{self.dataset_name}_{retrievers_str}{mc}{sm}{eq}_results.json', 'w') as f:\n",
        "            json.dump(correctness_results, f, indent=4)\n",
        "\n",
        "\n",
        "    def trace(self):\n",
        "        self.evaluate_retrievers([VECTOR_INDEX_RETREIVER], both=True)\n",
        "        self.evaluate_retrievers([BM25_INDEX_RETREIVER], both=True)\n",
        "        self.evaluate_retrievers([VECTOR_INDEX_RETREIVER, BM25_INDEX_RETREIVER])\n"
      ]
    },
    {
      "source": [
        "\n",
        "!pip install llama-index"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "I17jilwONwC3",
        "outputId": "b930ca2d-2fa8-463d-f19c-5af398f5f3e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.19-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.19 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.19-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.5-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.11.12)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.12-py3-none-any.whl.metadata (851 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading llama_index-0.12.19-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.19-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.20-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.5-py3-none-any.whl (39 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.1-py3-none-any.whl (4.8 kB)\n",
            "Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.1-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.12 llama-cloud-services-0.6.1 llama-index-0.12.19 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.0 llama-index-core-0.12.19 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.7 llama-index-llms-openai-0.3.20 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.5 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.1 marshmallow-3.26.1 mypy-extensions-1.0.0 pypdf-5.3.0 python-dotenv-1.0.1 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3el7_eKPNdd3"
      },
      "outputs": [],
      "source": [
        "datase_name = 'smos'\n",
        "use_mc = False\n",
        "use_summary = False\n",
        "use_similar_q = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr-J2R-8Ndd4",
        "outputId": "e7c2a36d-ae26-46cb-a42e-981b51bf3ab2",
        "colab": {
          "referenced_widgets": [
            "1dcebb4664994c559cc6f0045a57b273",
            "7273cf0c854a42258494a578fac9b00e",
            "081584368f9e482c802a5a39b289774a",
            "79a3f65435f14bdabb8dcf4d23155d98"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of indexed nodes: 395\n",
            "Number of requirements nodes: 67\n",
            "Extracting class info objects\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dcebb4664994c559cc6f0045a57b273",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes in the graph: 543\n",
            "Extracting call graph links\n",
            "Total: 2354\n",
            "Present: 2354, Absent: 0\n",
            "Number of nodes in the graph: 599\n",
            "Adding method calls\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7273cf0c854a42258494a578fac9b00e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating Documents:   0%|          | 0/599 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "081584368f9e482c802a5a39b289774a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79a3f65435f14bdabb8dcf4d23155d98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating Networkx Graph:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "nl2code_tracer = NL2CodeTracer(datase_name)\n",
        "nl2code_tracer.set_dataset_data(use_mc=use_mc, use_summary=use_summary, use_similar_q=use_similar_q)\n",
        "# nl2code_tracer.trace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPldEeoENdd5",
        "outputId": "c7249e08-c4d8-43be-c683-f292031776eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "What are the names of the classes that are related to the following use case requirement?\n",
            "\n",
            "Nome: Login\n",
            "Attori: Utente registrato\n",
            "Descrizione: Permette ad un utente di loggarsi al sistema\n",
            "Precondizioni:\n",
            "L’utente non è loggato al sistema L’utente possiede username e password Compila e sottomette il form per il login\n",
            "Sequenza degli eventi\n",
            "Utente\n",
            "Sistema\n",
            "1.\tVerifica che username e password abbiano lunghezza >=5. Se la condizione è rispettata passa al punto due, altrimenti notifica l'errore all'utente.\n",
            "2.\tCerca nell’archivio se username e password inseriti dall’utente sono presenti tra gli utenti loggabili\n",
            "3.\tSe la ricerca ha avuto successo l’utente viene loggato al sistema\n",
            "Postcondizioni:\n",
            "•\tIl sistema visualizza l’area di lavoro dell’Utente Registrato •\tInterruzione della connessione al server SMOS\n",
            "\n",
            "Set of similar requirements as context: \n",
            "**Requirement 1:** The system shall validate the username and password length to ensure they meet the minimum length requirement of 5 characters before attempting to authenticate the user.\n",
            "**Requirement 2:** The system shall authenticate a registered user by checking the provided username and password against the stored credentials in the database, and grant access to the system if the credentials match.\n",
            "\n",
            "Provide the answer in a list format and provide ONLY the list of class names as a JSON list.\n",
            "[<\"Class 1 Name\">, <\"Class 2 Name\">, ... <\"Class N Name\">] where N can be up to 10.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from prompts.templates import CLASS_TRACE_TEMPLATE\n",
        "\n",
        "\n",
        "q = CLASS_TRACE_TEMPLATE.format(requirement=nl2code_tracer.req_nodes[0].text)\n",
        "print(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leIxYx3xNdd6",
        "outputId": "c3c8d4ba-4546-40d8-d4a1-5172ffdc2743",
        "colab": {
          "referenced_widgets": [
            "168d5b1ca651438994e62976ef7b43b2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "168d5b1ca651438994e62976ef7b43b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "nl2code_tracer.set_retrivers([VECTOR_INDEX_RETREIVER])\n",
        "retrieved_nodes: List[NodeWithScore] = []\n",
        "for retriever in nl2code_tracer.retrievers:\n",
        "    retrieved_nodes.extend(retriever.retrieve(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YFE8M1iNdd6"
      },
      "outputs": [],
      "source": [
        "nl2code_tracer.set_retrivers([BM25_INDEX_RETREIVER])\n",
        "retrieved_nodes: List[NodeWithScore] = []\n",
        "for retriever in nl2code_tracer.retrievers:\n",
        "    retrieved_nodes.extend(retriever.retrieve(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm-Ld8lWNdd8",
        "outputId": "01e17307-126a-40a7-d3b2-4f461b0e7d27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ConnessioneWrapper', 'Utility']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(set([n.metadata['Class Name'] for n in retrieved_nodes]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaemwJWMNdd9",
        "outputId": "987b77b3-1c8c-47ef-b185-9c63bfc87554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['User',\n",
              " 'Role',\n",
              " 'UserListItem',\n",
              " 'managerUser',\n",
              " 'ServletLogin',\n",
              " 'ServletLogout',\n",
              " 'ServletAlterPersonalDate']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nl2code_tracer.req_file_to_node_map[nl2code_tracer.req_nodes[0].metadata['file_name']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZVG-P0oNdd-",
        "outputId": "df334687-e5d2-46ce-c308-b1bd0892628e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8459037741813271"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sim_result = await nl2code_tracer.get_semantic_score(q, nl2code_tracer.req_nodes[0].text)\n",
        "sim_result.score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEo-E85SNdd_"
      },
      "outputs": [],
      "source": [
        "# nl2code_tracer.sem_evaluator._embed_model.get_text_embedding(CLASS_TRACE_TEMPLATE.format(requirement=nl2code_tracer.req_nodes[1].text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDX1_ocONdeA",
        "outputId": "3a6561f7-22d4-4efa-f4d5-aff8a0113a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total:  67\n",
            "Before 67\n",
            "After 0\n",
            "Total:  58\n",
            "Before 58\n",
            "After 0\n",
            "Total:  131\n",
            "Before 131\n",
            "After 0\n",
            "Total:  139\n",
            "Before 139\n",
            "After 0\n"
          ]
        }
      ],
      "source": [
        "datasets = ['smos', 'eTour', 'iTrust', 'eANCI']\n",
        "for dataset in datasets:\n",
        "    with open(f'similar_requirements/{dataset}.pkl', 'rb') as f:\n",
        "        reqs = pickle.load(f)\n",
        "\n",
        "    print(\"Total: \", len(reqs))\n",
        "    print(\"Before\", len([1 for req in reqs if \", \".join(req.metadata['similar_queries']).startswith('Here are')]))\n",
        "    for req in reqs:\n",
        "        if \", \".join(req.metadata['similar_queries']).startswith('Here are'):\n",
        "            req.metadata['similar_queries'] = req.metadata['similar_queries'][1:]\n",
        "\n",
        "    with open(f'similar_requirements/{dataset}.pkl', 'wb') as f:\n",
        "        pickle.dump(reqs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAykU3wtNdeB",
        "outputId": "7097294d-2c5f-47dd-9502-074fb4646b16",
        "colab": {
          "referenced_widgets": [
            "cf88cd3f941b46e2ac3406a0bf710187",
            "6432603166cd4fc9abfc81f05accf0e4",
            "001603eb218e49e89370505721349e1a",
            "9c8869248179442eb212d7c94ae5eb45",
            "3c4d74fd16114cbf954fb2330f9fed73"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using LLM: meta-llama/Meta-Llama-3-70B-Instruct\n",
            "Using Embedding Model: BAAI/bge-m3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/Users/junaid/Anaconda/anaconda3/envs/ML/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of indexed nodes: 897\n",
            "Number of requirements nodes: 67\n",
            "Extracting class info objects\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf88cd3f941b46e2ac3406a0bf710187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes in the graph: 543\n",
            "Extracting call graph links\n",
            "Total: 2354\n",
            "Present: 2354, Absent: 0\n",
            "Number of nodes in the graph: 599\n",
            "Adding method calls\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6432603166cd4fc9abfc81f05accf0e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating Documents:   0%|          | 0/599 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "001603eb218e49e89370505721349e1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c8869248179442eb212d7c94ae5eb45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating Networkx Graph:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c4d74fd16114cbf954fb2330f9fed73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for vector_index_retriever_nl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 0/67 [00:00<?, ?Requirement/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from parameters import parse_args\n",
        "import os\n",
        "from constants import (\n",
        "    LLMsMap,\n",
        "    EmbeddingModelsMap,\n",
        ")\n",
        "\n",
        "from api_models import set_llm_and_embed\n",
        "args = parse_args()\n",
        "\n",
        "datasets = [\n",
        "    ('smos', 'bge_m3'),\n",
        "    ('eTour', 'bge_large'),\n",
        "    ('eANCI' 'bge_m3'),\n",
        "    ('iTrust', 'bge_large')\n",
        "]\n",
        "use_mcs = [True, False]\n",
        "use_summaries = [True, False]\n",
        "use_similar_qs = [True, False]\n",
        "\n",
        "for dataset, embed_model in datasets:\n",
        "    llm_name = LLMsMap[args.llm]\n",
        "    embed_model_name = EmbeddingModelsMap[embed_model]\n",
        "    print(f\"Using LLM: {llm_name}\")\n",
        "    print(f\"Using Embedding Model: {embed_model_name}\")\n",
        "\n",
        "    set_llm_and_embed(\n",
        "        llm_type=args.llm_type,\n",
        "        llm_name=llm_name,\n",
        "        embed_model_name=embed_model_name,\n",
        "    )\n",
        "    configs = [(mc, sm, eq) for mc in use_mcs for sm in use_summaries for eq in use_similar_qs]\n",
        "    for config in configs:\n",
        "        use_mc, use_summary, use_similar_q = config\n",
        "        nl2code_tracer = NL2CodeTracer(dataset)\n",
        "        nl2code_tracer.set_dataset_data(\n",
        "            use_mc=use_mc,\n",
        "            use_summary=use_summary,\n",
        "            use_similar_q=use_similar_q\n",
        "        )\n",
        "        nl2code_tracer.trace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo7kQmLsNdeC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "VENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}